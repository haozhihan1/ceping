#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“è¿ç§»å·¥å…·
ç”¨äºå¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„å’Œæ•°æ®ï¼Œç”Ÿæˆè¿ç§»æ–‡ä»¶
"""

import sqlite3
import json
import os
import csv
import zipfile
from datetime import datetime
import shutil
import sys

class DatabaseMigrator:
    """æ•°æ®åº“è¿ç§»å·¥å…·ç±»"""

    def __init__(self, db_path='new_questions.db', output_dir='migration_files'):
        self.db_path = db_path
        self.output_dir = output_dir
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def export_schema(self):
        """å¯¼å‡ºæ•°æ®åº“ç»“æ„"""
        print("æ­£åœ¨å¯¼å‡ºæ•°æ®åº“ç»“æ„...")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # è·å–æ‰€æœ‰è¡¨å
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()

        schema = {}

        for table in tables:
            table_name = table[0]
            print(f"  å¯¼å‡ºè¡¨ç»“æ„: {table_name}")

            # è·å–è¡¨ç»“æ„
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()

            # è·å–ç´¢å¼•ä¿¡æ¯
            cursor.execute(f"PRAGMA index_list({table_name})")
            indexes = cursor.fetchall()

            schema[table_name] = {
                'columns': [{
                    'name': col[1],
                    'type': col[2],
                    'not_null': bool(col[3]),
                    'default_value': col[4],
                    'primary_key': bool(col[5])
                } for col in columns],
                'indexes': [{
                    'name': idx[1],
                    'unique': bool(idx[2]),
                    'origin': idx[3]
                } for idx in indexes if not idx[1].startswith('sqlite_')]
            }

        conn.close()

        # ä¿å­˜ç»“æ„åˆ°æ–‡ä»¶
        schema_file = os.path.join(self.output_dir, f'schema_{self.timestamp}.json')
        with open(schema_file, 'w', encoding='utf-8') as f:
            json.dump(schema, f, ensure_ascii=False, indent=2)

        print(f"âœ“ æ•°æ®åº“ç»“æ„å·²å¯¼å‡ºåˆ°: {schema_file}")
        return schema_file

    def export_data(self):
        """å¯¼å‡ºæ‰€æœ‰è¡¨çš„æ•°æ®"""
        print("æ­£åœ¨å¯¼å‡ºæ•°æ®åº“æ•°æ®...")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # è·å–æ‰€æœ‰è¡¨å
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()

        data_files = []

        for table in tables:
            table_name = table[0]
            print(f"  å¯¼å‡ºè¡¨æ•°æ®: {table_name}")

            # è·å–è¡¨æ•°æ®
            cursor.execute(f"SELECT * FROM {table_name}")
            rows = cursor.fetchall()

            # è·å–åˆ—å
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = [col[1] for col in cursor.fetchall()]

            if rows:
                # ä¿å­˜ä¸ºCSVæ–‡ä»¶
                csv_file = os.path.join(self.output_dir, f'{table_name}_{self.timestamp}.csv')
                with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow(columns)  # å†™å…¥åˆ—å
                    writer.writerows(rows)   # å†™å…¥æ•°æ®

                data_files.append(csv_file)
                print(f"    âœ“ {table_name}: {len(rows)} æ¡è®°å½•")
            else:
                print(f"    - {table_name}: æ— æ•°æ®")

        conn.close()
        print(f"âœ“ æ•°æ®å¯¼å‡ºå®Œæˆï¼Œå…±å¤„ç† {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        return data_files

    def create_migration_script(self):
        """åˆ›å»ºæ•°æ®åº“è¿ç§»è„šæœ¬"""
        print("æ­£åœ¨åˆ›å»ºè¿ç§»è„šæœ¬...")

        script_content = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“è¿ç§»è„šæœ¬ - {self.timestamp}
ç”¨äºåœ¨æœåŠ¡å™¨ç«¯æ¢å¤æ•°æ®åº“ç»“æ„å’Œæ•°æ®
"""

import sqlite3
import csv
import os
import json
import sys
from datetime import datetime

class DatabaseRestorer:
    """æ•°æ®åº“æ¢å¤å·¥å…·ç±»"""

    def __init__(self, db_path, migration_dir):
        self.db_path = db_path
        self.migration_dir = migration_dir
        self.conn = None

    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        self.conn = sqlite3.connect(self.db_path)
        print(f"âœ“ è¿æ¥æ•°æ®åº“: {{self.db_path}}")

    def restore_schema(self, schema_file):
        """æ¢å¤æ•°æ®åº“ç»“æ„"""
        print("æ­£åœ¨æ¢å¤æ•°æ®åº“ç»“æ„...")

        with open(schema_file, 'r', encoding='utf-8') as f:
            schema = json.load(f)

        cursor = self.conn.cursor()

        for table_name, table_info in schema.items():
            print(f"  åˆ›å»ºè¡¨: {{table_name}}")

            # æ„å»ºCREATE TABLEè¯­å¥
            columns_sql = []
            primary_keys = []

            for col in table_info['columns']:
                col_sql = f"{{col['name']}} {{col['type']}}"

                if col['primary_key']:
                    primary_keys.append(col['name'])

                if col['not_null']:
                    col_sql += " NOT NULL"

                if col['default_value'] is not None:
                    col_sql += f" DEFAULT {{col['default_value']}}"

                columns_sql.append(col_sql)

            # æ·»åŠ ä¸»é”®çº¦æŸ
            if primary_keys:
                columns_sql.append(f"PRIMARY KEY ({', '.join(primary_keys)})")

            create_sql = f"CREATE TABLE IF NOT EXISTS {{table_name}} ({', '.join(columns_sql)})"
            cursor.execute(create_sql)

            # åˆ›å»ºç´¢å¼•
            for idx in table_info['indexes']:
                if idx['unique']:
                    cursor.execute(f"CREATE UNIQUE INDEX IF NOT EXISTS {{idx['name']}} ON {{table_name}}({{idx['origin']}})")
                else:
                    cursor.execute(f"CREATE INDEX IF NOT EXISTS {{idx['name']}} ON {{table_name}}({{idx['origin']}})")

        self.conn.commit()
        print("âœ“ æ•°æ®åº“ç»“æ„æ¢å¤å®Œæˆ")

    def restore_data(self, data_files):
        """æ¢å¤æ•°æ®"""
        print("æ­£åœ¨æ¢å¤æ•°æ®...")

        cursor = self.conn.cursor()

        for csv_file in data_files:
            table_name = os.path.basename(csv_file).split('_')[0]
            print(f"  æ¢å¤è¡¨æ•°æ®: {{table_name}}")

            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                columns = next(reader)  # è¯»å–åˆ—å

                # æ„å»ºINSERTè¯­å¥
                placeholders = ', '.join(['?'] * len(columns))
                insert_sql = f"INSERT OR REPLACE INTO {{table_name}} ({', '.join(columns)}) VALUES ({{placeholders}})"

                # æ‰¹é‡æ’å…¥æ•°æ®
                batch_size = 1000
                batch_data = []

                for row in reader:
                    batch_data.append(row)
                    if len(batch_data) >= batch_size:
                        cursor.executemany(insert_sql, batch_data)
                        batch_data = []

                # å¤„ç†å‰©ä½™æ•°æ®
                if batch_data:
                    cursor.executemany(insert_sql, batch_data)

                print(f"    âœ“ æ’å…¥ {{len(batch_data)}} æ¡è®°å½•")

        self.conn.commit()
        print("âœ“ æ•°æ®æ¢å¤å®Œæˆ")

    def initialize_admin(self):
        """åˆå§‹åŒ–ç®¡ç†å‘˜è´¦æˆ·"""
        print("æ­£åœ¨åˆå§‹åŒ–ç®¡ç†å‘˜è´¦æˆ·...")

        cursor = self.conn.cursor()

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç®¡ç†å‘˜
        cursor.execute("SELECT COUNT(*) FROM admins")
        count = cursor.fetchone()[0]

        if count == 0:
            cursor.execute("INSERT INTO admins (username, password) VALUES (?, ?)", (
                'haozhihan', 'Haozhihan010922ï¼'
            ))
            print("âœ“ é»˜è®¤ç®¡ç†å‘˜è´¦æˆ·å·²åˆ›å»º")
        else:
            print("âœ“ ç®¡ç†å‘˜è´¦æˆ·å·²å­˜åœ¨")

        self.conn.commit()

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            print("âœ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print(f"æ•°æ®åº“è¿ç§»è„šæœ¬ - {self.timestamp}")
    print("=" * 50)

    # é…ç½®è·¯å¾„
    db_path = os.environ.get('DATABASE_PATH', 'new_questions.db')
    migration_dir = os.path.dirname(__file__)

    # æŸ¥æ‰¾æœ€æ–°çš„è¿ç§»æ–‡ä»¶
    schema_files = [f for f in os.listdir(migration_dir) if f.startswith('schema_') and f.endswith('.json')]
    if not schema_files:
        print("âœ— æœªæ‰¾åˆ°schemaæ–‡ä»¶")
        return False

    latest_schema = max(schema_files)
    schema_file = os.path.join(migration_dir, latest_schema)

    # æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®æ–‡ä»¶
    timestamp = latest_schema.replace('schema_', '').replace('.json', '')
    data_files = [f for f in os.listdir(migration_dir)
                 if f.endswith(f'_{{timestamp}}.csv') and not f.startswith('schema_')]

    data_file_paths = [os.path.join(migration_dir, f) for f in data_files]

    print(f"æ‰¾åˆ°è¿ç§»æ–‡ä»¶:")
    print(f"  Schema: {{latest_schema}}")
    print(f"  æ•°æ®æ–‡ä»¶: {{len(data_files)}} ä¸ª")

    try:
        # æ‰§è¡Œè¿ç§»
        restorer = DatabaseRestorer(db_path, migration_dir)
        restorer.connect()
        restorer.restore_schema(schema_file)
        restorer.restore_data(data_file_paths)
        restorer.initialize_admin()
        restorer.close()

        print("âœ“ æ•°æ®åº“è¿ç§»å®Œæˆï¼")
        return True

    except Exception as e:
        print(f"âœ— è¿ç§»å¤±è´¥: {{e}}")
        return False

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
'''

        script_file = os.path.join(self.output_dir, f'migrate_{self.timestamp}.py')
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)

        print(f"âœ“ è¿ç§»è„šæœ¬å·²ç”Ÿæˆ: {script_file}")
        return script_file

    def create_archive(self):
        """åˆ›å»ºè¿ç§»æ–‡ä»¶å½’æ¡£"""
        print("æ­£åœ¨åˆ›å»ºè¿ç§»æ–‡ä»¶å½’æ¡£...")

        archive_name = f'database_migration_{self.timestamp}.zip'
        archive_path = os.path.join(self.output_dir, archive_name)

        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ æ‰€æœ‰è¿ç§»æ–‡ä»¶
            for file in os.listdir(self.output_dir):
                if file.startswith(('schema_', 'migrate_')) or file.endswith(f'_{self.timestamp}.csv'):
                    file_path = os.path.join(self.output_dir, file)
                    zipf.write(file_path, file)
                    print(f"  æ·»åŠ æ–‡ä»¶: {file}")

        print(f"âœ“ è¿ç§»å½’æ¡£å·²åˆ›å»º: {archive_path}")
        return archive_path

    def generate_report(self):
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        print("æ­£åœ¨ç”Ÿæˆè¿ç§»æŠ¥å‘Š...")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()

        report = {
            'migration_time': self.timestamp,
            'database_info': {},
            'tables': {}
        }

        total_records = 0
        for table in tables:
            table_name = table[0]
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            total_records += count

            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()

            report['tables'][table_name] = {
                'record_count': count,
                'column_count': len(columns),
                'columns': [col[1] for col in columns]
            }

        report['database_info'] = {
            'total_tables': len(tables),
            'total_records': total_records,
            'database_size': os.path.getsize(self.db_path) if os.path.exists(self.db_path) else 0
        }

        conn.close()

        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f'migration_report_{self.timestamp}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"âœ“ è¿ç§»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("æ•°æ®åº“è¿ç§»å·¥å…·")
    print("=" * 50)

    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists('new_questions.db'):
        print("âœ— æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: new_questions.db")
        return False

    try:
        migrator = DatabaseMigrator()

        # æ‰§è¡Œè¿ç§»æ­¥éª¤
        schema_file = migrator.export_schema()
        data_files = migrator.export_data()
        script_file = migrator.create_migration_script()
        archive_file = migrator.create_archive()
        report_file = migrator.generate_report()

        print("\n" + "=" * 50)
        print("è¿ç§»å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  ğŸ“ è¿ç§»ç›®å½•: {migrator.output_dir}")
        print(f"  ğŸ“„ Schemaæ–‡ä»¶: {os.path.basename(schema_file)}")
        print(f"  ğŸ“Š æ•°æ®æ–‡ä»¶: {len(data_files)} ä¸ª")
        print(f"  ğŸ”§ è¿ç§»è„šæœ¬: {os.path.basename(script_file)}")
        print(f"  ğŸ“¦ å½’æ¡£æ–‡ä»¶: {os.path.basename(archive_file)}")
        print(f"  ğŸ“‹ æŠ¥å‘Šæ–‡ä»¶: {os.path.basename(report_file)}")
        print("=" * 50)

        print("\néƒ¨ç½²è¯´æ˜:")
        print("1. å°†å½’æ¡£æ–‡ä»¶ä¸Šä¼ åˆ°æœåŠ¡å™¨")
        print("2. åœ¨æœåŠ¡å™¨ä¸Šè§£å‹å½’æ¡£æ–‡ä»¶")
        print("3. è¿è¡Œè¿ç§»è„šæœ¬: python migrate_*.py")
        print("4. éªŒè¯æ•°æ®åº“æ˜¯å¦æ­£ç¡®æ¢å¤")

        return True

    except Exception as e:
        print(f"âœ— è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
